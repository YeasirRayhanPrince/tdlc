{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# LSTM and RNN code derived from the following github repo: https://github.com/TobiasLee/Text-Classification\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from tensorflow.contrib.rnn import BasicLSTMCell\n",
    "from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn\n",
    "import tensorflow as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR FINAL TEMPORAL PRECISION EXPERIMENTS\n",
    "input311File = \"../../Comparison/TEMPORAL_PRECISION/DEEPCRIME/Dataset/1d/complaint_matrix_1d.pkl\"\n",
    "inputCrimeFile = \"../../Comparison/TEMPORAL_PRECISION/DEEPCRIME/Dataset/1d/crime_matrix_1d.pkl\"\n",
    "\n",
    "with open(input311File, 'rb') as pickle_file:\n",
    "    anomaly = pickle.load(pickle_file)\n",
    "\n",
    "with open(inputCrimeFile,'rb') as pickle_file:\n",
    "    content = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_upto_MARCH = pickle.load(open(\"../Chicago Dataset/test_upto_MARCH.pkl\", \"rb\"))\n",
    "test_xmas = pickle.load(open(\"../Chicago Dataset/test_xmas.pkl\", \"rb\"))\n",
    "test_easter = pickle.load(open(\"../Chicago Dataset/test_upto_MARCH.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(365, 308)\n",
      "(365, 308)\n"
     ]
    }
   ],
   "source": [
    "inp = np.reshape(content, (365, 77*4))\n",
    "inpA = np.reshape(anomaly, (365, 77*4))\n",
    "\n",
    "inp1 = np.where(inp > 0, 1, 0)\n",
    "print(np.shape(inp1))\n",
    "print(np.shape(inpA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(365, 308)\n",
      "(365, 308)\n",
      "(91, 308)\n",
      "(91, 308)\n",
      "(365, 308)\n",
      "(91, 308)\n"
     ]
    }
   ],
   "source": [
    "# Train test split for the above data\n",
    "\n",
    "# size = int(len(inp)*0.8)\n",
    "size = 91\n",
    "x_train = inp\n",
    "y_train = inp1\n",
    "\n",
    "# x_test = np.reshape(test_upto_MARCH, (len(test_upto_MARCH), 77*4))\n",
    "x_test = np.reshape(test_easter, (len(test_easter), 77*4))\n",
    "# x_test = np.reshape(test_xmas, (len(test_xmas), 77*4))\n",
    "y_test = np.where(x_test > 0, 1, 0)\n",
    "\n",
    "x_train2 = inpA\n",
    "x_test2 = inpA[:size]\n",
    "\n",
    "print(np.shape(x_train))\n",
    "print(np.shape(y_train))\n",
    "print(np.shape(x_test))\n",
    "print(np.shape(y_test))\n",
    "print(np.shape(x_train2))\n",
    "print(np.shape(x_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(inputs, attention_size, time_major=False, return_alphas=False):\n",
    "    \"\"\"\n",
    "    Attention mechanism layer which reduces RNN/Bi-RNN outputs with Attention vector.\n",
    "    The idea was proposed in the article by Z. Yang et al., \"Hierarchical Attention Networks\n",
    "     for Document Classification\", 2016: http://www.aclweb.org/anthology/N16-1174.\n",
    "    Variables notation is also inherited from the article\n",
    "    Args:\n",
    "        inputs: The Attention inputs.\n",
    "            Matches outputs of RNN/Bi-RNN layer (not final state):\n",
    "                In case of RNN, this must be RNN outputs `Tensor`:\n",
    "                    If time_major == False (default), this must be a tensor of shape:\n",
    "                        `[batch_size, max_time, cell.output_size]`.\n",
    "                    If time_major == True, this must be a tensor of shape:\n",
    "                        `[max_time, batch_size, cell.output_size]`.\n",
    "                In case of Bidirectional RNN, this must be a tuple (outputs_fw, outputs_bw) containing the forward and\n",
    "                the backward RNN outputs `Tensor`.\n",
    "                    If time_major == False (default),\n",
    "                        outputs_fw is a `Tensor` shaped:\n",
    "                        `[batch_size, max_time, cell_fw.output_size]`\n",
    "                        and outputs_bw is a `Tensor` shaped:\n",
    "                        `[batch_size, max_time, cell_bw.output_size]`.\n",
    "                    If time_major == True,\n",
    "                        outputs_fw is a `Tensor` shaped:\n",
    "                        `[max_time, batch_size, cell_fw.output_size]`\n",
    "                        and outputs_bw is a `Tensor` shaped:\n",
    "                        `[max_time, batch_size, cell_bw.output_size]`.\n",
    "        attention_size: Linear size of the Attention weights.\n",
    "        time_major: The shape format of the `inputs` Tensors.\n",
    "            If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n",
    "            If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n",
    "            Using `time_major = True` is a bit more efficient because it avoids\n",
    "            transposes at the beginning and end of the RNN calculation.  However,\n",
    "            most TensorFlow data is batch-major, so by default this function\n",
    "            accepts input and emits output in batch-major form.\n",
    "        return_alphas: Whether to return attention coefficients variable along with layer's output.\n",
    "            Used for visualization purpose.\n",
    "    Returns:\n",
    "        The Attention output `Tensor`.\n",
    "        In case of RNN, this will be a `Tensor` shaped:\n",
    "            `[batch_size, cell.output_size]`.\n",
    "        In case of Bidirectional RNN, this will be a `Tensor` shaped:\n",
    "            `[batch_size, cell_fw.output_size + cell_bw.output_size]`.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(inputs, tuple):\n",
    "        # In case of Bi-RNN, concatenate the forward and the backward RNN outputs.\n",
    "        inputs = tf.concat(inputs, 2)\n",
    "\n",
    "    if time_major:\n",
    "        # (T,B,D) => (B,T,D)\n",
    "        inputs = tf.array_ops.transpose(inputs, [1, 0, 2])\n",
    "\n",
    "    hidden_size = inputs.shape[2].value  # D value - hidden size of the RNN layer\n",
    "\n",
    "    # Trainable parameters\n",
    "    w_omega = tf.Variable(tf.random_normal([hidden_size, attention_size], stddev=0.1))\n",
    "    b_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "    u_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "\n",
    "    with tf.name_scope('v'):\n",
    "        # Applying fully connected layer with non-linear activation to each of the B*T timestamps;\n",
    "        #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n",
    "        v = tf.tanh(tf.tensordot(inputs, w_omega, axes=1) + b_omega)\n",
    "\n",
    "    # For each of the timestamps its vector of size A from `v` is reduced with `u` vector\n",
    "    vu = tf.tensordot(v, u_omega, axes=1, name='vu')  # (B,T) shape\n",
    "    alphas = tf.nn.softmax(vu, name='alphas')  # (B,T) shape\n",
    "\n",
    "    # Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape\n",
    "    output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)\n",
    "\n",
    "    if not return_alphas:\n",
    "        return output\n",
    "    else:\n",
    "        return output, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_dataset(x_test, y_test, dev_ratio):\n",
    "    \"\"\"split test dataset to test and dev set with ratio \"\"\"\n",
    "    test_size = len(x_test)\n",
    "    print(test_size)\n",
    "    dev_size = (int)(test_size * dev_ratio)\n",
    "    print(dev_size)\n",
    "    x_dev = x_test[:dev_size]\n",
    "    x_test = x_test[dev_size:]\n",
    "    y_dev = y_test[:dev_size]\n",
    "    y_test = y_test[dev_size:]\n",
    "    return x_test, x_dev, y_test, y_dev, dev_size, test_size - dev_size\n",
    "\n",
    "\n",
    "def fill_feed_dict(data_X, data_Y, batch_size):\n",
    "    \"\"\"Generator to yield batches\"\"\"\n",
    "    # Shuffle data first.\n",
    "    shuffled_X, shuffled_Y = shuffle(data_X, data_Y)\n",
    "    # print(\"before shuffle: \", data_Y[:10])\n",
    "    # print(data_X.shape[0])\n",
    "    # perm = np.random.permutation(data_X.shape[0])\n",
    "    # data_X = data_X[perm]\n",
    "    # shuffled_Y = data_Y[perm]\n",
    "    # print(\"after shuffle: \", shuffled_Y[:10])\n",
    "    for idx in range(data_X.shape[0] // batch_size):\n",
    "        x_batch = shuffled_X[batch_size * idx: batch_size * (idx + 1)]\n",
    "        y_batch = shuffled_Y[batch_size * idx: batch_size * (idx + 1)]\n",
    "        yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "# # default setting\n",
    "# MAX_DOCUMENT_LENGTH = 128\n",
    "# EMBEDDING_SIZE = 128\n",
    "# HIDDEN_SIZE = 64\n",
    "# ATTENTION_SIZE = 64\n",
    "# lr = 5e-4\n",
    "# learning_rate=0.001\n",
    "# hidden_dim = 250\n",
    "# BATCH_SIZE = 4\n",
    "# KEEP_PROB = 1.0\n",
    "# LAMBDA = 0.0001\n",
    "# MAX_LABEL = 77*4\n",
    "# # MAX_LABEL = 5*4\n",
    "# epochs = 10\n",
    "# latent_dim = 8\n",
    "# # n_batches = 1\n",
    "# timeSize = 10\n",
    "max_len=10\n",
    "\n",
    "# # test setting\n",
    "HIDDEN_SIZE = 64 * 2\n",
    "lr = 5e-4\n",
    "BATCH_SIZE = 10 \n",
    "KEEP_PROB = 0.8\n",
    "MAX_LABEL = 77*4\n",
    "# MAX_LABEL = 5*4\n",
    "epochs = 50\n",
    "timeSize = 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_label_hot(prediction, threshold=0.5):\n",
    "    prediction = tf.cast(prediction, tf.float32)\n",
    "    threshold = float(threshold)\n",
    "    return tf.cast(tf.greater(prediction, threshold), tf.int64)\n",
    "\n",
    "def get_metrics(labels_tensor, one_hot_prediction, num_classes):\n",
    "    metrics = {}\n",
    "    with tf.variable_scope(\"metrics\"):\n",
    "        for scope in [\"train\", \"val\"]:\n",
    "            with tf.variable_scope(scope):\n",
    "                with tf.variable_scope(\"accuracy\"):\n",
    "                    accuracy, accuracy_update = tf.metrics.accuracy(\n",
    "                        tf.cast(one_hot_prediction, tf.int32),\n",
    "                        labels_tensor,\n",
    "                    )\n",
    "                metrics[scope] = {\n",
    "                    \"accuracy\": accuracy,\n",
    "                    \"updates\": tf.group(accuracy_update),\n",
    "                }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 20, 308)\n",
      "(?, 20, 128)\n",
      "(?, 20)\n",
      "<unknown>\n"
     ]
    }
   ],
   "source": [
    "# RNN based architecture with Attention\n",
    "# https://github.com/TobiasLee/Text-Classification\n",
    "\n",
    "tf.reset_default_graph()\n",
    "batch_x = tf.placeholder(tf.float32, [None,timeSize,MAX_LABEL])\n",
    "anomaly_x = tf.placeholder(tf.float32, [None,timeSize,MAX_LABEL])\n",
    "batch_y = tf.placeholder(tf.float32, [None, MAX_LABEL])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "rnn_outputs1, _ = tf.nn.dynamic_rnn(BasicLSTMCell(HIDDEN_SIZE),\n",
    "                        inputs=batch_x, dtype=tf.float32,scope='BLSTM_1')\n",
    "\n",
    "rnn_outputs2, _ = tf.nn.dynamic_rnn(BasicLSTMCell(HIDDEN_SIZE),\n",
    "                        inputs=anomaly_x, dtype=tf.float32,scope='BLSTM_2')\n",
    "\n",
    "# weights for balance-outs\n",
    "weight_out = tf.Variable(tf.truncated_normal([2], stddev=0.1))\n",
    "weight_soft = tf.nn.softmax(weight_out)\n",
    "\n",
    "inputAdd = weight_soft[0]*rnn_outputs1 + weight_soft[1]*rnn_outputs2\n",
    "print(batch_x.shape)\n",
    "print(inputAdd.shape)\n",
    "rnn_outputs, _ = tf.nn.dynamic_rnn(BasicLSTMCell(HIDDEN_SIZE),\n",
    "                        inputs=inputAdd, dtype=tf.float32,scope='BLSTM_3')\n",
    "# fw_outputs, bw_outputs = rnn_outputs\n",
    "\n",
    "\n",
    "# # Attention\n",
    "# attention_output, alphas = attention(rnn_outputs, ATTENTION_SIZE, return_alphas=True)\n",
    "# drop = tf.nn.dropout(attention_output, keep_prob)\n",
    "# shape = drop.get_shape()\n",
    "# print(shape)\n",
    "# # Fully connected layer（dense layer)\n",
    "# W = tf.Variable(tf.truncated_normal([shape[1].value, MAX_LABEL], stddev=0.1))\n",
    "# b = tf.Variable(tf.constant(0., shape=[MAX_LABEL]))\n",
    "# y_hat = tf.nn.xw_plus_b(drop, W, b)\n",
    "W = tf.Variable(tf.random_normal([HIDDEN_SIZE], stddev=0.1))\n",
    "H = rnn_outputs  # (batch_size, seq_len, HIDDEN_SIZE)\n",
    "M = tf.tanh(H)  # M = tanh(H)  (batch_size, seq_len, HIDDEN_SIZE)\n",
    "\n",
    "alpha = tf.nn.softmax(tf.reshape(tf.matmul(tf.reshape(M, [-1, HIDDEN_SIZE]),\n",
    "                                                tf.reshape(W, [-1, 1])),\n",
    "                                      (-1, timeSize )))  # batch_size x seq_len\n",
    "\n",
    "print(alpha.shape)\n",
    "r = tf.matmul(tf.transpose(H, [0, 2, 1]),\n",
    "              tf.reshape(alpha, [-1, timeSize, 1]))\n",
    "r = tf.squeeze(r)\n",
    "h_star = tf.tanh(r)  # (batch , HIDDEN_SIZE\n",
    "\n",
    "h_drop = tf.nn.dropout(h_star, keep_prob)\n",
    "shape = h_drop.get_shape()\n",
    "# print(h_star.shape)\n",
    "# Fully connected layer（dense layer)\n",
    "FC_W = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, MAX_LABEL], stddev=0.1))\n",
    "FC_b = tf.Variable(tf.constant(0., shape=[MAX_LABEL]))\n",
    "y_hat2 = tf.nn.xw_plus_b(h_drop, FC_W, FC_b)\n",
    "print(y_hat2.shape)\n",
    "FC_W2 = tf.Variable(tf.truncated_normal([MAX_LABEL, MAX_LABEL], stddev=0.1))\n",
    "FC_b2 = tf.Variable(tf.constant(0., shape=[MAX_LABEL]))\n",
    "y_hat = tf.nn.xw_plus_b(y_hat2, FC_W2, FC_b2)\n",
    "\n",
    "loss =  tf.nn.l2_loss(y_hat-batch_y) +0.001*tf.nn.l2_loss(FC_W)+0.001*tf.nn.l2_loss(FC_W2) + 0.0001*tf.nn.l2_loss(W)\n",
    "# loss =   tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_hat, labels=batch_y)) +0.001*tf.nn.l2_loss(FC_W)+0.001*tf.nn.l2_loss(FC_W2) + 0.0001*tf.nn.l2_loss(W)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "prediction = tf.sigmoid(y_hat)\n",
    "one_hot_prediction = multi_label_hot(prediction)\n",
    "\n",
    "accuracy  =  get_metrics(batch_y,one_hot_prediction,77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "Variables: name (type shape) [size]\n",
      "---------\n",
      "BLSTM_1/basic_lstm_cell/kernel:0 (float32_ref 436x512) [223232, bytes: 892928]\n",
      "BLSTM_1/basic_lstm_cell/bias:0 (float32_ref 512) [512, bytes: 2048]\n",
      "BLSTM_2/basic_lstm_cell/kernel:0 (float32_ref 436x512) [223232, bytes: 892928]\n",
      "BLSTM_2/basic_lstm_cell/bias:0 (float32_ref 512) [512, bytes: 2048]\n",
      "Variable:0 (float32_ref 2) [2, bytes: 8]\n",
      "BLSTM_3/basic_lstm_cell/kernel:0 (float32_ref 256x512) [131072, bytes: 524288]\n",
      "BLSTM_3/basic_lstm_cell/bias:0 (float32_ref 512) [512, bytes: 2048]\n",
      "Variable_1:0 (float32_ref 128) [128, bytes: 512]\n",
      "Variable_2:0 (float32_ref 128x308) [39424, bytes: 157696]\n",
      "Variable_3:0 (float32_ref 308) [308, bytes: 1232]\n",
      "Variable_4:0 (float32_ref 308x308) [94864, bytes: 379456]\n",
      "Variable_5:0 (float32_ref 308) [308, bytes: 1232]\n",
      "Total size of variables: 714106\n",
      "Total bytes of variables: 2856424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file checkpointDir already exists.\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "!mkdir checkpointDir\n",
    "\n",
    "# Model Parameters\n",
    "slim = tf.contrib.slim\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())\n",
    "def model_summary():\n",
    "    model_vars = tf.trainable_variables()\n",
    "    slim.model_analyzer.analyze_vars(model_vars, print_info=True)\n",
    "    \n",
    "model_summary()\n",
    "\n",
    "# To store training and test results for visualization\n",
    "\n",
    "tr = []\n",
    "ts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized! \n",
      "Start trainning\n",
      "Epoch 1 start !\n",
      "TRain ::  7469.7144  : micro  0.6840546024367242  : macro 0.6569480710741321  :  1.8908653259277344\n",
      "Epoch 2 start !\n",
      "TRain ::  3527.7075  : micro  0.7225874450677052  : macro 0.693882503137584  :  1.7146399021148682\n",
      "Epoch 3 start !\n",
      "TRain ::  3484.1575  : micro  0.723207034146686  : macro 0.6948715637400052  :  1.7445487976074219\n",
      "Epoch 4 start !\n",
      "TRain ::  3383.9949  : micro  0.7227961764989862  : macro 0.6952419022815036  :  1.7096703052520752\n",
      "Epoch 5 start !\n",
      "TRain ::  3347.1863  : micro  0.7217967547908799  : macro 0.6942702334281582  :  1.6461844444274902\n",
      "Epoch 6 start !\n",
      "TRain ::  3340.1423  : micro  0.7210695932410068  : macro 0.6937265172876937  :  1.586057424545288\n",
      "Epoch 7 start !\n",
      "TRain ::  3314.952  : micro  0.7206492491971183  : macro 0.693516564118016  :  1.5635044574737549\n",
      "Epoch 8 start !\n",
      "TRain ::  3310.2417  : micro  0.7201757876706403  : macro 0.6932219072885124  :  1.713627815246582\n",
      "Epoch 9 start !\n",
      "TRain ::  3282.0427  : micro  0.7199965478553552  : macro 0.693541213700904  :  1.8829588890075684\n",
      "Epoch 10 start !\n",
      "TRain ::  3266.2266  : micro  0.719576628220181  : macro 0.6927251907456576  :  1.5693392753601074\n",
      "Epoch 11 start !\n",
      "TRain ::  3253.5933  : micro  0.7192547866026733  : macro 0.6927485884153467  :  1.543079137802124\n",
      "Epoch 12 start !\n",
      "TRain ::  3232.2776  : micro  0.7189423530926727  : macro 0.6928392316273083  :  1.6795258522033691\n",
      "Epoch 13 start !\n",
      "TRain ::  3229.8608  : micro  0.7186530185347118  : macro 0.6927070481148032  :  1.5588047504425049\n",
      "Epoch 14 start !\n",
      "TRain ::  3198.2378  : micro  0.7181769170866956  : macro 0.6922932975710281  :  1.9645395278930664\n",
      "Epoch 15 start !\n",
      "TRain ::  3201.0542  : micro  0.7172392564598047  : macro 0.6913494688280897  :  1.7598748207092285\n",
      "Epoch 16 start !\n",
      "TRain ::  3203.8362  : micro  0.7171356895404202  : macro 0.6914720192210989  :  1.7391822338104248\n",
      "Epoch 17 start !\n",
      "TRain ::  3166.3818  : micro  0.7168862602055599  : macro 0.6913718946080752  :  2.1234378814697266\n",
      "Epoch 18 start !\n",
      "TRain ::  3158.5366  : micro  0.716836473915562  : macro 0.6912894421063868  :  1.785675287246704\n",
      "Epoch 19 start !\n",
      "TRain ::  3123.9768  : micro  0.7157034343297024  : macro 0.6904315950355138  :  2.0063607692718506\n",
      "Epoch 20 start !\n",
      "TRain ::  3086.1301  : micro  0.7143378329076979  : macro 0.6890547181087696  :  2.105785846710205\n",
      "Epoch 21 start !\n",
      "TRain ::  3061.827  : micro  0.7146464646464646  : macro 0.689732758008012  :  1.7475292682647705\n",
      "Epoch 22 start !\n",
      "TRain ::  3059.1443  : micro  0.7138540705695159  : macro 0.688974742235578  :  1.7964744567871094\n",
      "Epoch 23 start !\n",
      "TRain ::  3040.4023  : micro  0.7146680670133654  : macro 0.6896711375282982  :  1.6754419803619385\n",
      "Epoch 24 start !\n",
      "TRain ::  3052.6755  : micro  0.7140226078434685  : macro 0.6892598247002333  :  1.7648382186889648\n",
      "Epoch 25 start !\n",
      "TRain ::  3021.017  : micro  0.7137947050627033  : macro 0.689225023393609  :  1.72813081741333\n",
      "Epoch 26 start !\n",
      "TRain ::  3045.5095  : micro  0.7134184952845721  : macro 0.688802086571269  :  1.524174451828003\n",
      "Epoch 27 start !\n",
      "TRain ::  3047.2952  : micro  0.7128687819442336  : macro 0.6884493932687823  :  1.7206823825836182\n",
      "Epoch 28 start !\n",
      "TRain ::  3063.206  : micro  0.712853589646305  : macro 0.6884165091218137  :  1.5665218830108643\n",
      "Epoch 29 start !\n",
      "TRain ::  3093.5925  : micro  0.7128111718275654  : macro 0.688374804473523  :  1.7973895072937012\n",
      "Epoch 30 start !\n",
      "TRain ::  3111.2588  : micro  0.7128073753223333  : macro 0.6883975186376091  :  2.1888375282287598\n",
      "Epoch 31 start !\n",
      "TRain ::  3120.1787  : micro  0.7124461347747991  : macro 0.6879304117178916  :  2.215087413787842\n",
      "Epoch 32 start !\n",
      "TRain ::  3069.3074  : micro  0.7130409845053435  : macro 0.6886098737540797  :  2.194659948348999\n",
      "Epoch 33 start !\n",
      "TRain ::  3006.3594  : micro  0.7133940620782726  : macro 0.6889966254328392  :  2.122408628463745\n",
      "Epoch 34 start !\n",
      "TRain ::  2972.8696  : micro  0.7131197938161697  : macro 0.6888557819468444  :  2.159125328063965\n",
      "Epoch 35 start !\n",
      "TRain ::  2954.5042  : micro  0.7130360032362459  : macro 0.688760454680316  :  2.334562301635742\n",
      "Epoch 36 start !\n",
      "TRain ::  2958.2134  : micro  0.7130096564265328  : macro 0.6889698214171897  :  2.024523973464966\n",
      "Epoch 37 start !\n",
      "TRain ::  2934.0015  : micro  0.7129720079496076  : macro 0.6888037130358583  :  1.7542250156402588\n",
      "Epoch 38 start !\n",
      "TRain ::  2910.6782  : micro  0.713503772568041  : macro 0.6892038326379141  :  1.8825798034667969\n",
      "Epoch 39 start !\n",
      "TRain ::  2922.9646  : micro  0.7128423000883355  : macro 0.6887323135556787  :  1.980689287185669\n",
      "Epoch 40 start !\n",
      "TRain ::  2902.199  : micro  0.7118361995057743  : macro 0.6877033985993661  :  1.5930867195129395\n",
      "Epoch 41 start !\n",
      "TRain ::  2890.5308  : micro  0.7123506512069822  : macro 0.6881788921977638  :  1.4742107391357422\n",
      "Epoch 42 start !\n",
      "TRain ::  2878.094  : micro  0.7126891088442812  : macro 0.6886808013519665  :  1.4841194152832031\n",
      "Epoch 43 start !\n",
      "TRain ::  2896.2993  : micro  0.711724948574787  : macro 0.6879695192241605  :  1.4630532264709473\n",
      "Epoch 44 start !\n",
      "TRain ::  2922.8792  : micro  0.7114158163265306  : macro 0.687611652343528  :  1.450103521347046\n",
      "Epoch 45 start !\n",
      "TRain ::  2894.2095  : micro  0.7109944834755781  : macro 0.6872299682049304  :  1.6272404193878174\n",
      "Epoch 46 start !\n",
      "TRain ::  2898.8557  : micro  0.7104062167661763  : macro 0.6867613799933419  :  1.4543023109436035\n",
      "Epoch 47 start !\n",
      "TRain ::  2863.6646  : micro  0.7104164920807843  : macro 0.6866251506499428  :  1.463623285293579\n",
      "Epoch 48 start !\n",
      "TRain ::  2843.3398  : micro  0.7097821086529212  : macro 0.6862541162384034  :  1.463423728942871\n",
      "Epoch 49 start !\n",
      "TRain ::  2819.2517  : micro  0.7089840975373327  : macro 0.685575894370486  :  1.4493663311004639\n",
      "Epoch 50 start !\n",
      "TRain ::  2846.811  : micro  0.7090306032934257  : macro 0.6857097110050357  :  1.4705164432525635\n"
     ]
    }
   ],
   "source": [
    "#sess.run(tf.global_variables_initializer())\n",
    "print(\"Initialized! \")\n",
    "target_names = ['a','b','c','d']\n",
    "print(\"Start trainning\")\n",
    "start = time.time()\n",
    "\n",
    "testA = 0\n",
    "predsAr = []\n",
    "\n",
    "# Training for 50 epochs\n",
    "for e in range(epochs):   \n",
    "# for e in range(5):\n",
    "\n",
    "    epoch_start = time.time()\n",
    "    print(\"Epoch %d start !\" % (e + 1))\n",
    "    # for x_batch, y_batch in zip(x_train, y_train, BATCH_SIZE):\n",
    "    err = []\n",
    "    preds = []\n",
    "    trues = []\n",
    "\n",
    "    # batch sizes for crimes and anomaly and prediction\n",
    "    x_batch1 =[]\n",
    "    x_batch2 = []\n",
    "    y_batch1 = []\n",
    "    \n",
    "    # Recording error, prediction, truth values(for F1-scores)\n",
    "    for i in range(len(x_train)-80):                                      # 292 - 80 = 212 \n",
    "        i+=80\n",
    "        x_batch = x_train[i:min(len(x_train)-1,timeSize+(i))]             # i=0 -> 0-min(292-1, 10+1)=11  -> 0-11 = 12  # i=1 -> 1-min(292-1, 10+1)=11  -> 1-11 = 10\n",
    "        x_anomaly = x_train2[i:min(len(x_train)-1,timeSize+(i))]          # i=0 -> 0-min(292-1, 10+1)=11  -> 0-11 = 12  # i=1 -> 1-min(292-1, 10+1)=11  -> 1-11 = 10\n",
    "        if len(x_batch) < timeSize:\n",
    "          continue\n",
    "        x_batch = x_batch\n",
    "        x_anomaly = x_anomaly\n",
    "        y_batch = x_train[min(len(x_train)-1,timeSize+(i))].T\n",
    "        x_batch1.append(x_batch)\n",
    "        x_batch2.append(x_anomaly)\n",
    "        y_batch1.append(y_batch)\n",
    "        if (i+1)% BATCH_SIZE >0:\n",
    "          continue\n",
    "        # print(np.array(x_batch1).shape)\n",
    "        fd = {batch_x: x_batch1, anomaly_x: x_batch2, batch_y: y_batch1, keep_prob: KEEP_PROB}\n",
    "        # print(y_batch)\n",
    "        \n",
    "        l, _, oht = sess.run([loss, optimizer, one_hot_prediction], feed_dict=fd)\n",
    "        \n",
    "        for j in range(BATCH_SIZE):\n",
    "          # print(oht.shape)\n",
    "          preds.extend(np.array(oht[j]).reshape(-1,4))\n",
    "          trues.extend(np.array(y_batch1[j]).reshape(-1,4))\n",
    "        x_batch1 =[]\n",
    "        y_batch1 = []\n",
    "        x_batch2 = []\n",
    "\n",
    "        # sess.run(optimizer,feed_dict=fd)\n",
    "        err.append(l)\n",
    "    # print(sess.run(loss))\n",
    "    epoch_finish = time.time()\n",
    "    # print(preds)\n",
    "    preds = np.array(preds)\n",
    "    trues = np.array(trues)\n",
    "\n",
    "    # f1 = f1_score(y_true=y_batch, y_pred=oht, average='weighted')\n",
    "    f1 = f1_score(y_true=np.where(trues>0,1,0), y_pred=np.where(preds>0,1,0), average='micro')\n",
    "    f2 = f1_score(y_true=np.where(trues>0,1,0), y_pred=np.where(preds>0,1,0), average='macro')\n",
    "    tr.append([f1,f2])\n",
    "    # print(f1)\n",
    "    print(\"TRain :: \",np.mean(err),\" : micro \",f1,\" : macro\",f2,\" : \",epoch_finish-epoch_start)\n",
    "    # print(classification_report(y_true=trues,y_pred=preds,target_names=target_names))\n",
    "\n",
    "    ########### Predictions on test data and storing info for visualization ###########\n",
    "    if True:\n",
    "      preds = []\n",
    "      trues = []\n",
    "      x_batch1 =[]\n",
    "      y_batch1 = []\n",
    "      x_batch2 = []\n",
    "      err = []\n",
    "      for i in range(len(x_test)):\n",
    "          # i+=100\n",
    "          x_batch = x_test[i:min(len(x_test)-1,timeSize+(i))]\n",
    "          x_anomaly = x_test2[i:min(len(x_test)-1,timeSize+(i))]\n",
    "          if len(x_batch) < timeSize:\n",
    "            continue\n",
    "          x_batch = x_batch\n",
    "          x_anomaly = x_anomaly\n",
    "          y_batch = x_test[min(len(x_test)-1,timeSize+(i))].T\n",
    "          x_batch1.append(x_batch)\n",
    "          x_batch2.append(x_anomaly)\n",
    "          y_batch1.append(y_batch)\n",
    "          if (i+1)% BATCH_SIZE >0:\n",
    "            continue\n",
    "          fd = {batch_x: x_batch1,anomaly_x:x_batch2, batch_y: y_batch1, keep_prob: KEEP_PROB}\n",
    "          \n",
    "          l, acc,oht,weightSupport = sess.run([loss, accuracy,one_hot_prediction,weight_soft], feed_dict=fd)\n",
    "          \n",
    "          err.append(l)\n",
    "          # sess.run(optimizer,feed_dict=fd)\n",
    "          for j in range(BATCH_SIZE):\n",
    "            # print(oht.shape)\n",
    "            preds.extend(np.array(oht[j]).reshape(-1,4))\n",
    "            trues.extend(np.array(y_batch1[j]).reshape(-1,4))\n",
    "\n",
    "          x_batch1 =[]\n",
    "          y_batch1 = []\n",
    "          x_batch2 = []\n",
    "\n",
    "      preds = np.array(preds)\n",
    "      trues = np.array(trues)\n",
    "\n",
    "    #   ##################################### BY ALIF #####################################\n",
    "    #   # # save preds and trues in a pickle file\n",
    "    #   # with open('../Chicago Dataset/preds_before_threshold_1d_only_crime_data.pkl', 'wb') as f:\n",
    "    #   #   pickle.dump(preds, f)\n",
    "    #   # with open('../Chicago Dataset/trues_before_threshold_1d_only_crime_data.pkl', 'wb') as f:\n",
    "    #   #   pickle.dump(trues, f)\n",
    "      \n",
    "    #   # save preds and trues in a pickle file\n",
    "    #   with open('../../Comparison/TEMPORAL_PRECISION/DEEPCRIME/Outputs/1d_wo_external/preds_before_threshold.pkl', 'wb') as f:\n",
    "    #     pickle.dump(preds, f)\n",
    "    #   with open('../../Comparison/TEMPORAL_PRECISION/DEEPCRIME/Outputs/1d_wo_external/trues_before_threshold.pkl', 'wb') as f:\n",
    "    #     pickle.dump(trues, f)\n",
    "    #   ###################################################################################\n",
    "\n",
    "      y_pred = preds\n",
    "      y_true = trues\n",
    "\n",
    "    #   ##################################### BY ALIF #####################################\n",
    "    #   # # save preds and trues in a pickle file\n",
    "    #   # with open('../Chicago Dataset/preds_after_threshold_1d_only_crime_data.pkl', 'wb') as f:\n",
    "    #   #   pickle.dump(y_pred, f)\n",
    "    #   # with open('../Chicago Dataset/trues_after_threshold_1d_only_crime_data.pkl', 'wb') as f:\n",
    "    #   #   pickle.dump(y_true, f)\n",
    "\n",
    "      \n",
    "      # save preds and trues in a pickle file\n",
    "      with open('../HOLIDAY_ANALYSIS/pred_reg_easter.pkl', 'wb') as f:\n",
    "        pickle.dump(y_pred, f)\n",
    "      with open('../HOLIDAY_ANALYSIS/true_reg_easter.pkl', 'wb') as f:\n",
    "        pickle.dump(y_true, f)\n",
    "    #   ###################################################################################\n",
    "\n",
    "      # with open('preds_NYC.pkl', 'wb') as f:\n",
    "      #   pickle.dump(preds, f)\n",
    "      # with open('trues_NYC.pkl', 'wb') as f:\n",
    "      #   pickle.dump(trues, f)\n",
    "\n",
    "      # f1 = f1_score(y_true=y_batch, y_pred=oht, average='weighted')\n",
    "      f1 = f1_score(y_true=np.where(trues>0,1,0), y_pred=np.where(preds>0,1,0), average='micro')\n",
    "      f2 = f1_score(y_true=np.where(trues>0,1,0), y_pred=np.where(preds>0,1,0), average='macro')\n",
    "\n",
    "      if testA < f1:\n",
    "        testA=f1\n",
    "        save_path = saver.save(sess, \"./modelM/model\"+str(f1)[:5]+\".ckpt\")\n",
    "        # print(classification_report(y_true=trues,y_pred=preds,target_names=target_names))\n",
    "        predsAr.append(preds)\n",
    "      ts.append([f1,f2])\n",
    "      # print(f1)\n",
    "      # print(np.mean(err),\" : micro \",f1,\" : macro\",f2,\" : \")\n",
    "      # print(weightSupport)\n",
    "      # print(classification_report(y_true=trues,y_pred=preds,target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
