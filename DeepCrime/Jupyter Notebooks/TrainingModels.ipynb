{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "E9Mz_1lmhE8x"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.14.0\n",
            "  Downloading tensorflow-1.14.0-cp37-cp37m-win_amd64.whl (68.3 MB)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==1.14.0) (1.51.1)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "  Using cached tensorboard-1.14.0-py3-none-any.whl (3.1 MB)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==1.14.0) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==1.14.0) (1.1.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==1.14.0) (1.18.5)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==1.14.0) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==1.14.0) (2.1.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==1.14.0) (0.2.2)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "  Using cached tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488 kB)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==1.14.0) (3.19.6)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==1.14.0) (1.12.1)\n",
            "Requirement already satisfied: wheel>=0.26 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==1.14.0) (0.33.6)\n",
            "Requirement already satisfied: astor>=0.6.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==1.14.0) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==1.14.0) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow==1.14.0) (1.14.0)\n",
            "Requirement already satisfied: h5py in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (2.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.4.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (65.6.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (2.2.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (5.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.7.4.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from werkzeug>=0.11.15->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (2.1.1)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Keyring is skipped due to an exception: 'EntryPoints' object has no attribute 'get'\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "    WARNING: Ignoring invalid distribution -ip (c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "    WARNING: Ignoring invalid distribution -ip (c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-intel 2.11.0 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "tensorflow-intel 2.11.0 requires tensorboard<2.12,>=2.11, but you have tensorboard 1.14.0 which is incompatible.\n",
            "tensorflow-intel 2.11.0 requires tensorflow-estimator<2.12,>=2.11.0, but you have tensorflow-estimator 1.14.0 which is incompatible.\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "WARNING: You are using pip version 21.2.4; however, version 23.0.1 is available.\n",
            "You should consider upgrading via the 'C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboard==1.14.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.14.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.14.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.14.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard==1.14.0) (2.2.2)\n",
            "Requirement already satisfied: grpcio>=1.6.3 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard==1.14.0) (1.51.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard==1.14.0) (3.19.6)\n",
            "Requirement already satisfied: six>=1.10.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard==1.14.0) (1.14.0)\n",
            "Requirement already satisfied: wheel>=0.26 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard==1.14.0) (0.33.6)\n",
            "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard==1.14.0) (1.18.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard==1.14.0) (3.4.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard==1.14.0) (65.6.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard==1.14.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from markdown>=2.6.8->tensorboard==1.14.0) (5.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard==1.14.0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard==1.14.0) (3.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from werkzeug>=0.11.15->tensorboard==1.14.0) (2.1.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Keyring is skipped due to an exception: 'EntryPoints' object has no attribute 'get'\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
            "WARNING: You are using pip version 21.2.4; however, version 23.0.1 is available.\n",
            "You should consider upgrading via the 'C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "# !pip uninstall tensorflow -y\n",
        "!pip install tensorflow==1.14.0\n",
        "!pip install tensorboard==1.14.0 tensorflow-estimator==1.14.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "M4hUt1m5f7iD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "# LSTM and RNN code derived from the following github repo: https://github.com/TobiasLee/Text-Classification\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "from tensorflow.contrib.rnn import BasicLSTMCell\n",
        "from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn\n",
        "import tensorflow as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "5Rw4kUmTHw8G"
      },
      "outputs": [],
      "source": [
        "# # (365x78x4) matrices, as mentioned in the preprocessing steps\n",
        "\n",
        "# # input311File = 'matrices311_NYC'\n",
        "# # inputCrimeFile = 'matricesCR_NYC'\n",
        "\n",
        "# input311File = \"../Chicago Dataset/complaint_matrix.pkl\"\n",
        "# inputCrimeFile = \"../Chicago Dataset/crime_matrix.pkl\"\n",
        "\n",
        "# # input311File = \"../Chicago Dataset/complaint_matrix_4h.pkl\"\n",
        "# # inputCrimeFile = \"../Chicago Dataset/crime_matrix_4h.pkl\"\n",
        "\n",
        "# # input311File = \"../Chicago Dataset/complaint_matrix_12h.pkl\"\n",
        "# # inputCrimeFile = \"../Chicago Dataset/crime_matrix_12h.pkl\"\n",
        "\n",
        "# # input311File = \"../Chicago Dataset/complaint_matrix_7d.pkl\"\n",
        "# # inputCrimeFile = \"../Chicago Dataset/crime_matrix_7d.pkl\"\n",
        "\n",
        "# with open(input311File, 'rb') as pickle_file:\n",
        "#     anomaly = pickle.load(pickle_file)\n",
        "\n",
        "# with open(inputCrimeFile,'rb') as pickle_file:\n",
        "#     content = pickle.load(pickle_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FOR FINAL TEMPORAL PRECISION EXPERIMENTS\n",
        "input311File = \"../../Comparison/TEMPORAL_PRECISION/DEEPCRIME/Dataset/1d/complaint_matrix_1d.pkl\"\n",
        "inputCrimeFile = \"../../Comparison/TEMPORAL_PRECISION/DEEPCRIME/Dataset/1d/crime_matrix_1d.pkl\"\n",
        "\n",
        "# input311File = \"../../Comparison/TEMPORAL_PRECISION/Deepcrime/Dataset/12h/complaint_matrix_12h.pkl\"\n",
        "# inputCrimeFile = \"../../Comparison/TEMPORAL_PRECISION/Deepcrime/Dataset/12h/crime_matrix_12h.pkl\"\n",
        "\n",
        "# input311File = \"../../Comparison/TEMPORAL_PRECISION/Deepcrime/Dataset/6h/complaint_matrix_6h.pkl\"\n",
        "# inputCrimeFile = \"../../Comparison/TEMPORAL_PRECISION/Deepcrime/Dataset/6h/crime_matrix_6h.pkl\"\n",
        "\n",
        "# input311File = \"../../Comparison/TEMPORAL_PRECISION/Deepcrime/4h/complaint_matrix_4h.pkl\"\n",
        "# inputCrimeFile = \"../../Comparison/TEMPORAL_PRECISION/Deepcrime/4h/crime_matrix_4h.pkl\"\n",
        "\n",
        "with open(input311File, 'rb') as pickle_file:\n",
        "    anomaly = pickle.load(pickle_file)\n",
        "\n",
        "with open(inputCrimeFile,'rb') as pickle_file:\n",
        "    content = pickle.load(pickle_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(365, 77, 4)\n",
            "(365, 77, 4)\n"
          ]
        }
      ],
      "source": [
        "print(np.shape(anomaly))\n",
        "print(np.shape(content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "BpLNLOnjYQxf"
      },
      "outputs": [],
      "source": [
        "# # for NYC Dataset\n",
        "\n",
        "# # Preprocessing the crime and anomaly matrices\n",
        "# # To remove the 0th row of each of the (78x4) matrices\n",
        "# # ,since they contain data events for which geographical data was not available\n",
        "# # as mentioned in the preprocessing code\n",
        "\n",
        "# dat = []    # crime matrix\n",
        "# dat2 = []   # anomaly matrix\n",
        "# for i in range(len(content)):\n",
        "#   a = []\n",
        "#   b = []\n",
        "#   # for j in range(77):\n",
        "#   for j in range(5):\n",
        "#     a.extend(content[i][j+1])\n",
        "#     b.extend(anomaly[i][j+1])\n",
        "#     # print(a)\n",
        "#   dat.append(a)\n",
        "#   dat2.append(b)\n",
        "\n",
        "# inp = np.array(dat)\n",
        "# inp1 = np.where(inp>0,1,0)\n",
        "# inpA = np.array(dat2)\n",
        "\n",
        "# print(np.shape(inp1))\n",
        "# print(np.shape(inpA))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(365, 308)\n",
            "(365, 308)\n"
          ]
        }
      ],
      "source": [
        "# for Chicago dataset\n",
        "\n",
        "# reshape content to (365, 77 x 4) matrix\n",
        "# reshape anomaly to (365, 77 x 4) matrix\n",
        "\n",
        "# 1 day\n",
        "inp = np.reshape(content, (365, 77*4))\n",
        "inpA = np.reshape(anomaly, (365, 77*4))\n",
        "\n",
        "# 4 hours\n",
        "# inp = np.reshape(content, (365*6, 77*4))\n",
        "# inpA = np.reshape(anomaly, (365*6, 77*4))\n",
        "\n",
        "# 12 hours\n",
        "# inp = np.reshape(content, (365*2, 77*4))\n",
        "# inpA = np.reshape(anomaly, (365*2, 77*4))\n",
        "\n",
        "# 1 week\n",
        "# inp = np.reshape(content, (365//7+1, 77*4))\n",
        "# inpA = np.reshape(anomaly, (365//7+1, 77*4))\n",
        "\n",
        "# if the value is greater than 0, set it to 1\n",
        "inp1 = np.where(inp > 0, 1, 0)\n",
        "\n",
        "print(np.shape(inp1))\n",
        "print(np.shape(inpA))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "FZbeSPW4cFX_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(292, 308)\n",
            "(292, 308)\n",
            "(73, 308)\n",
            "(73, 308)\n",
            "(292, 308)\n",
            "(73, 308)\n"
          ]
        }
      ],
      "source": [
        "# Train test split for the above data\n",
        "\n",
        "size = int(len(inp)*0.8)\n",
        "x_train = inp[:size]\n",
        "y_train = inp1[:size]\n",
        "x_test = inp[size:]\n",
        "y_test = inp1[size:]\n",
        "x_train2 = inpA[:size]\n",
        "x_test2 = inpA[size:]\n",
        "\n",
        "print(np.shape(x_train))\n",
        "print(np.shape(y_train))\n",
        "print(np.shape(x_test))\n",
        "print(np.shape(y_test))\n",
        "print(np.shape(x_train2))\n",
        "print(np.shape(x_test2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "8Gfz6cDfgQrs"
      },
      "outputs": [],
      "source": [
        "def attention(inputs, attention_size, time_major=False, return_alphas=False):\n",
        "    \"\"\"\n",
        "    Attention mechanism layer which reduces RNN/Bi-RNN outputs with Attention vector.\n",
        "    The idea was proposed in the article by Z. Yang et al., \"Hierarchical Attention Networks\n",
        "     for Document Classification\", 2016: http://www.aclweb.org/anthology/N16-1174.\n",
        "    Variables notation is also inherited from the article\n",
        "    Args:\n",
        "        inputs: The Attention inputs.\n",
        "            Matches outputs of RNN/Bi-RNN layer (not final state):\n",
        "                In case of RNN, this must be RNN outputs `Tensor`:\n",
        "                    If time_major == False (default), this must be a tensor of shape:\n",
        "                        `[batch_size, max_time, cell.output_size]`.\n",
        "                    If time_major == True, this must be a tensor of shape:\n",
        "                        `[max_time, batch_size, cell.output_size]`.\n",
        "                In case of Bidirectional RNN, this must be a tuple (outputs_fw, outputs_bw) containing the forward and\n",
        "                the backward RNN outputs `Tensor`.\n",
        "                    If time_major == False (default),\n",
        "                        outputs_fw is a `Tensor` shaped:\n",
        "                        `[batch_size, max_time, cell_fw.output_size]`\n",
        "                        and outputs_bw is a `Tensor` shaped:\n",
        "                        `[batch_size, max_time, cell_bw.output_size]`.\n",
        "                    If time_major == True,\n",
        "                        outputs_fw is a `Tensor` shaped:\n",
        "                        `[max_time, batch_size, cell_fw.output_size]`\n",
        "                        and outputs_bw is a `Tensor` shaped:\n",
        "                        `[max_time, batch_size, cell_bw.output_size]`.\n",
        "        attention_size: Linear size of the Attention weights.\n",
        "        time_major: The shape format of the `inputs` Tensors.\n",
        "            If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n",
        "            If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n",
        "            Using `time_major = True` is a bit more efficient because it avoids\n",
        "            transposes at the beginning and end of the RNN calculation.  However,\n",
        "            most TensorFlow data is batch-major, so by default this function\n",
        "            accepts input and emits output in batch-major form.\n",
        "        return_alphas: Whether to return attention coefficients variable along with layer's output.\n",
        "            Used for visualization purpose.\n",
        "    Returns:\n",
        "        The Attention output `Tensor`.\n",
        "        In case of RNN, this will be a `Tensor` shaped:\n",
        "            `[batch_size, cell.output_size]`.\n",
        "        In case of Bidirectional RNN, this will be a `Tensor` shaped:\n",
        "            `[batch_size, cell_fw.output_size + cell_bw.output_size]`.\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(inputs, tuple):\n",
        "        # In case of Bi-RNN, concatenate the forward and the backward RNN outputs.\n",
        "        inputs = tf.concat(inputs, 2)\n",
        "\n",
        "    if time_major:\n",
        "        # (T,B,D) => (B,T,D)\n",
        "        inputs = tf.array_ops.transpose(inputs, [1, 0, 2])\n",
        "\n",
        "    hidden_size = inputs.shape[2].value  # D value - hidden size of the RNN layer\n",
        "\n",
        "    # Trainable parameters\n",
        "    w_omega = tf.Variable(tf.random_normal([hidden_size, attention_size], stddev=0.1))\n",
        "    b_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
        "    u_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
        "\n",
        "    with tf.name_scope('v'):\n",
        "        # Applying fully connected layer with non-linear activation to each of the B*T timestamps;\n",
        "        #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n",
        "        v = tf.tanh(tf.tensordot(inputs, w_omega, axes=1) + b_omega)\n",
        "\n",
        "    # For each of the timestamps its vector of size A from `v` is reduced with `u` vector\n",
        "    vu = tf.tensordot(v, u_omega, axes=1, name='vu')  # (B,T) shape\n",
        "    alphas = tf.nn.softmax(vu, name='alphas')  # (B,T) shape\n",
        "\n",
        "    # Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape\n",
        "    output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)\n",
        "\n",
        "    if not return_alphas:\n",
        "        return output\n",
        "    else:\n",
        "        return output, alphas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "BQDCuDUWgWQE"
      },
      "outputs": [],
      "source": [
        "\n",
        "def split_dataset(x_test, y_test, dev_ratio):\n",
        "    \"\"\"split test dataset to test and dev set with ratio \"\"\"\n",
        "    test_size = len(x_test)\n",
        "    print(test_size)\n",
        "    dev_size = (int)(test_size * dev_ratio)\n",
        "    print(dev_size)\n",
        "    x_dev = x_test[:dev_size]\n",
        "    x_test = x_test[dev_size:]\n",
        "    y_dev = y_test[:dev_size]\n",
        "    y_test = y_test[dev_size:]\n",
        "    return x_test, x_dev, y_test, y_dev, dev_size, test_size - dev_size\n",
        "\n",
        "\n",
        "def fill_feed_dict(data_X, data_Y, batch_size):\n",
        "    \"\"\"Generator to yield batches\"\"\"\n",
        "    # Shuffle data first.\n",
        "    shuffled_X, shuffled_Y = shuffle(data_X, data_Y)\n",
        "    # print(\"before shuffle: \", data_Y[:10])\n",
        "    # print(data_X.shape[0])\n",
        "    # perm = np.random.permutation(data_X.shape[0])\n",
        "    # data_X = data_X[perm]\n",
        "    # shuffled_Y = data_Y[perm]\n",
        "    # print(\"after shuffle: \", shuffled_Y[:10])\n",
        "    for idx in range(data_X.shape[0] // batch_size):\n",
        "        x_batch = shuffled_X[batch_size * idx: batch_size * (idx + 1)]\n",
        "        y_batch = shuffled_Y[batch_size * idx: batch_size * (idx + 1)]\n",
        "        yield x_batch, y_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "VZMRnuWSgf3P"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "\n",
        "# # default setting\n",
        "# MAX_DOCUMENT_LENGTH = 128\n",
        "# EMBEDDING_SIZE = 128\n",
        "# HIDDEN_SIZE = 64\n",
        "# ATTENTION_SIZE = 64\n",
        "# lr = 5e-4\n",
        "# learning_rate=0.001\n",
        "# hidden_dim = 250\n",
        "# BATCH_SIZE = 4\n",
        "# KEEP_PROB = 1.0\n",
        "# LAMBDA = 0.0001\n",
        "# MAX_LABEL = 77*4\n",
        "# # MAX_LABEL = 5*4\n",
        "# epochs = 10\n",
        "# latent_dim = 8\n",
        "# # n_batches = 1\n",
        "# timeSize = 10\n",
        "max_len=10\n",
        "\n",
        "# # test setting\n",
        "HIDDEN_SIZE = 64 * 2\n",
        "lr = 5e-4\n",
        "BATCH_SIZE = 10 \n",
        "KEEP_PROB = 0.8\n",
        "MAX_LABEL = 77*4\n",
        "# MAX_LABEL = 5*4\n",
        "epochs = 50\n",
        "timeSize = 20 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "gABUU4alh137"
      },
      "outputs": [],
      "source": [
        "def multi_label_hot(prediction, threshold=0.5):\n",
        "    prediction = tf.cast(prediction, tf.float32)\n",
        "    threshold = float(threshold)\n",
        "    return tf.cast(tf.greater(prediction, threshold), tf.int64)\n",
        "\n",
        "def get_metrics(labels_tensor, one_hot_prediction, num_classes):\n",
        "    metrics = {}\n",
        "    with tf.variable_scope(\"metrics\"):\n",
        "        for scope in [\"train\", \"val\"]:\n",
        "            with tf.variable_scope(scope):\n",
        "                with tf.variable_scope(\"accuracy\"):\n",
        "                    accuracy, accuracy_update = tf.metrics.accuracy(\n",
        "                        tf.cast(one_hot_prediction, tf.int32),\n",
        "                        labels_tensor,\n",
        "                    )\n",
        "                metrics[scope] = {\n",
        "                    \"accuracy\": accuracy,\n",
        "                    \"updates\": tf.group(accuracy_update),\n",
        "                }\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "RTRzGFiLgmfp"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'Tensor' object does not support item assignment",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-13-1b3991784d59>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m##################### BY ALIF #####################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m# Turning off the 311 complaint data processing layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mweight_soft\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[0mweight_soft\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m####################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: 'Tensor' object does not support item assignment"
          ]
        }
      ],
      "source": [
        "# Bi-LSTM based architecture with Attention\n",
        "# https://github.com/TobiasLee/Text-Classification\n",
        "\n",
        "tf.reset_default_graph()\n",
        "batch_x = tf.placeholder(tf.float32, [None,timeSize,MAX_LABEL])\n",
        "anomaly_x = tf.placeholder(tf.float32, [None,timeSize,MAX_LABEL])\n",
        "batch_y = tf.placeholder(tf.float32, [None, MAX_LABEL])\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "rnn_outputs1, _ = bi_rnn(BasicLSTMCell(HIDDEN_SIZE),\n",
        "                        BasicLSTMCell(HIDDEN_SIZE),\n",
        "                        inputs=batch_x, dtype=tf.float32,scope='BLSTM_1')\n",
        "fw_outputs1, bw_outputs1 = rnn_outputs1\n",
        "\n",
        "rnn_outputs2, _ = bi_rnn(BasicLSTMCell(HIDDEN_SIZE),\n",
        "                        BasicLSTMCell(HIDDEN_SIZE),\n",
        "                        inputs=anomaly_x, dtype=tf.float32,scope='BLSTM_2')\n",
        "fw_outputs2, bw_outputs2 = rnn_outputs2\n",
        "\n",
        "# weights for balance outs\n",
        "weight_out = tf.Variable(tf.truncated_normal([4], stddev=0.1))\n",
        "weight_soft = tf.nn.softmax(weight_out)\n",
        "\n",
        "##################### BY ALIF #####################\n",
        "# Turning off the 311 complaint data processing layer\n",
        "weight_soft[1] = 0\n",
        "weight_soft[3] = 0\n",
        "####################################################\n",
        "\n",
        "inputAdd = weight_soft[0]*fw_outputs1 + weight_soft[1]**fw_outputs2 + weight_soft[2]*bw_outputs1 + weight_soft[3]*bw_outputs2\n",
        "print(batch_x.shape)\n",
        "print(inputAdd.shape)\n",
        "rnn_outputs, _ = bi_rnn(BasicLSTMCell(HIDDEN_SIZE),\n",
        "                        BasicLSTMCell(HIDDEN_SIZE),\n",
        "                        inputs=inputAdd, dtype=tf.float32,scope='BLSTM_3')\n",
        "fw_outputs, bw_outputs = rnn_outputs\n",
        "\n",
        "# # Attention\n",
        "# attention_output, alphas = attention(rnn_outputs, ATTENTION_SIZE, return_alphas=True)\n",
        "# drop = tf.nn.dropout(attention_output, keep_prob)\n",
        "# shape = drop.get_shape()\n",
        "# print(shape)\n",
        "# # Fully connected layer（dense layer)\n",
        "# W = tf.Variable(tf.truncated_normal([shape[1].value, MAX_LABEL], stddev=0.1))\n",
        "# b = tf.Variable(tf.constant(0., shape=[MAX_LABEL]))\n",
        "# y_hat = tf.nn.xw_plus_b(drop, W, b)\n",
        "W = tf.Variable(tf.random_normal([HIDDEN_SIZE], stddev=0.1))\n",
        "H = fw_outputs + bw_outputs  # (batch_size, seq_len, HIDDEN_SIZE)\n",
        "M = tf.tanh(H)  # M = tanh(H)  (batch_size, seq_len, HIDDEN_SIZE)\n",
        "\n",
        "alpha = tf.nn.softmax(tf.reshape(tf.matmul(tf.reshape(M, [-1, HIDDEN_SIZE]),\n",
        "                                                tf.reshape(W, [-1, 1])),\n",
        "                                      (-1, timeSize )))  # batch_size x seq_len\n",
        "\n",
        "print(alpha.shape)\n",
        "r = tf.matmul(tf.transpose(H, [0, 2, 1]),\n",
        "              tf.reshape(alpha, [-1, timeSize, 1]))\n",
        "r = tf.squeeze(r)\n",
        "h_star = tf.tanh(r)  # (batch , HIDDEN_SIZE\n",
        "\n",
        "h_drop = tf.nn.dropout(h_star, keep_prob)\n",
        "shape = h_drop.get_shape()\n",
        "# print(h_star.shape)\n",
        "# Fully connected layer（dense layer)\n",
        "FC_W = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, MAX_LABEL], stddev=0.1))\n",
        "FC_b = tf.Variable(tf.constant(0., shape=[MAX_LABEL]))\n",
        "y_hat2 = tf.nn.xw_plus_b(h_drop, FC_W, FC_b)\n",
        "print(y_hat2.shape)\n",
        "FC_W2 = tf.Variable(tf.truncated_normal([MAX_LABEL, MAX_LABEL], stddev=0.1))\n",
        "FC_b2 = tf.Variable(tf.constant(0., shape=[MAX_LABEL]))\n",
        "y_hat = tf.nn.xw_plus_b(y_hat2, FC_W2, FC_b2)\n",
        "\n",
        "# ######## LOSS FUNCTIONS ######\n",
        "\n",
        "# This loss function is used to predict the actual number of crime occurences, hence the L2 loss\n",
        "loss =  tf.nn.l2_loss(y_hat-batch_y) +0.001*tf.nn.l2_loss(FC_W)+0.001*tf.nn.l2_loss(FC_W2) + 0.0001*tf.nn.l2_loss(W)\n",
        "\n",
        "# Uncomment this, if you just want the binary predictions, not actual crime numbers\n",
        "# loss =   tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_hat, labels=batch_y)) +0.001*tf.nn.l2_loss(FC_W)+0.001*tf.nn.l2_loss(FC_W2) + 0.0001*tf.nn.l2_loss(W)\n",
        "\n",
        "# ######## LOSS FUNCTIONS ######\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
        "\n",
        "# optimization\n",
        "# loss_to_minimize = loss\n",
        "# tvars = tf.trainable_variables()\n",
        "# gradients = tf.gradients(loss_to_minimize, tvars, aggregation_method=tf.AggregationMethod.EXPERIMENTAL_TREE)\n",
        "# grads, global_norm = tf.clip_by_global_norm(gradients, 1.0)\n",
        "\n",
        "# global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "# optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
        "# train_op = optimizer.apply_gradients(zip(grads, tvars), global_step=global_step,\n",
        "#                                                 name='train_step')\n",
        "\n",
        "# Accuracy metric\n",
        "# prediction = tf.argmax(tf.nn.softmax(y_hat), 1)\n",
        "# accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, tf.argmax(batch_y, 1)), tf.float32))\n",
        "\n",
        "prediction = tf.sigmoid(y_hat)\n",
        "one_hot_prediction = multi_label_hot(prediction)\n",
        "\n",
        "accuracy  =  get_metrics(batch_y, one_hot_prediction, 77)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "FNBTW_SUOX77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(?, 20, 308)\n",
            "(?, 20, 128)\n",
            "(?, 20)\n",
            "<unknown>\n"
          ]
        }
      ],
      "source": [
        "# RNN based architecture with Attention\n",
        "# https://github.com/TobiasLee/Text-Classification\n",
        "\n",
        "tf.reset_default_graph()\n",
        "batch_x = tf.placeholder(tf.float32, [None,timeSize,MAX_LABEL])\n",
        "anomaly_x = tf.placeholder(tf.float32, [None,timeSize,MAX_LABEL])\n",
        "batch_y = tf.placeholder(tf.float32, [None, MAX_LABEL])\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "rnn_outputs1, _ = tf.nn.dynamic_rnn(BasicLSTMCell(HIDDEN_SIZE),\n",
        "                        inputs=batch_x, dtype=tf.float32,scope='BLSTM_1')\n",
        "\n",
        "rnn_outputs2, _ = tf.nn.dynamic_rnn(BasicLSTMCell(HIDDEN_SIZE),\n",
        "                        inputs=anomaly_x, dtype=tf.float32,scope='BLSTM_2')\n",
        "\n",
        "# weights for balance-outs\n",
        "weight_out = tf.Variable(tf.truncated_normal([2], stddev=0.1))\n",
        "weight_soft = tf.nn.softmax(weight_out)\n",
        "\n",
        "################## BY ALIF ##################\n",
        "# Turning off the 311 complaint data processing layer\n",
        "# weight_soft[1] = 0\n",
        "rnn_outputs2 = tf.zeros_like(rnn_outputs1)\n",
        "################## BY ALIF ##################\n",
        "\n",
        "inputAdd = weight_soft[0]*rnn_outputs1 + weight_soft[1]*rnn_outputs2\n",
        "print(batch_x.shape)\n",
        "print(inputAdd.shape)\n",
        "rnn_outputs, _ = tf.nn.dynamic_rnn(BasicLSTMCell(HIDDEN_SIZE),\n",
        "                        inputs=inputAdd, dtype=tf.float32,scope='BLSTM_3')\n",
        "# fw_outputs, bw_outputs = rnn_outputs\n",
        "\n",
        "\n",
        "# # Attention\n",
        "# attention_output, alphas = attention(rnn_outputs, ATTENTION_SIZE, return_alphas=True)\n",
        "# drop = tf.nn.dropout(attention_output, keep_prob)\n",
        "# shape = drop.get_shape()\n",
        "# print(shape)\n",
        "# # Fully connected layer（dense layer)\n",
        "# W = tf.Variable(tf.truncated_normal([shape[1].value, MAX_LABEL], stddev=0.1))\n",
        "# b = tf.Variable(tf.constant(0., shape=[MAX_LABEL]))\n",
        "# y_hat = tf.nn.xw_plus_b(drop, W, b)\n",
        "W = tf.Variable(tf.random_normal([HIDDEN_SIZE], stddev=0.1))\n",
        "H = rnn_outputs  # (batch_size, seq_len, HIDDEN_SIZE)\n",
        "M = tf.tanh(H)  # M = tanh(H)  (batch_size, seq_len, HIDDEN_SIZE)\n",
        "\n",
        "alpha = tf.nn.softmax(tf.reshape(tf.matmul(tf.reshape(M, [-1, HIDDEN_SIZE]),\n",
        "                                                tf.reshape(W, [-1, 1])),\n",
        "                                      (-1, timeSize )))  # batch_size x seq_len\n",
        "\n",
        "print(alpha.shape)\n",
        "r = tf.matmul(tf.transpose(H, [0, 2, 1]),\n",
        "              tf.reshape(alpha, [-1, timeSize, 1]))\n",
        "r = tf.squeeze(r)\n",
        "h_star = tf.tanh(r)  # (batch , HIDDEN_SIZE\n",
        "\n",
        "h_drop = tf.nn.dropout(h_star, keep_prob)\n",
        "shape = h_drop.get_shape()\n",
        "# print(h_star.shape)\n",
        "# Fully connected layer（dense layer)\n",
        "FC_W = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, MAX_LABEL], stddev=0.1))\n",
        "FC_b = tf.Variable(tf.constant(0., shape=[MAX_LABEL]))\n",
        "y_hat2 = tf.nn.xw_plus_b(h_drop, FC_W, FC_b)\n",
        "print(y_hat2.shape)\n",
        "FC_W2 = tf.Variable(tf.truncated_normal([MAX_LABEL, MAX_LABEL], stddev=0.1))\n",
        "FC_b2 = tf.Variable(tf.constant(0., shape=[MAX_LABEL]))\n",
        "y_hat = tf.nn.xw_plus_b(y_hat2, FC_W2, FC_b2)\n",
        "\n",
        "loss =  tf.nn.l2_loss(y_hat-batch_y) +0.001*tf.nn.l2_loss(FC_W)+0.001*tf.nn.l2_loss(FC_W2) + 0.0001*tf.nn.l2_loss(W)\n",
        "# loss =   tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_hat, labels=batch_y)) +0.001*tf.nn.l2_loss(FC_W)+0.001*tf.nn.l2_loss(FC_W2) + 0.0001*tf.nn.l2_loss(W)\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
        "prediction = tf.sigmoid(y_hat)\n",
        "one_hot_prediction = multi_label_hot(prediction)\n",
        "\n",
        "accuracy  =  get_metrics(batch_y,one_hot_prediction,77)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "56Uuo9D8Uxdi"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A subdirectory or file checkpointDir already exists.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------\n",
            "Variables: name (type shape) [size]\n",
            "---------\n",
            "BLSTM_1/basic_lstm_cell/kernel:0 (float32_ref 436x512) [223232, bytes: 892928]\n",
            "BLSTM_1/basic_lstm_cell/bias:0 (float32_ref 512) [512, bytes: 2048]\n",
            "BLSTM_2/basic_lstm_cell/kernel:0 (float32_ref 436x512) [223232, bytes: 892928]\n",
            "BLSTM_2/basic_lstm_cell/bias:0 (float32_ref 512) [512, bytes: 2048]\n",
            "Variable:0 (float32_ref 2) [2, bytes: 8]\n",
            "BLSTM_3/basic_lstm_cell/kernel:0 (float32_ref 256x512) [131072, bytes: 524288]\n",
            "BLSTM_3/basic_lstm_cell/bias:0 (float32_ref 512) [512, bytes: 2048]\n",
            "Variable_1:0 (float32_ref 128) [128, bytes: 512]\n",
            "Variable_2:0 (float32_ref 128x308) [39424, bytes: 157696]\n",
            "Variable_3:0 (float32_ref 308) [308, bytes: 1232]\n",
            "Variable_4:0 (float32_ref 308x308) [94864, bytes: 379456]\n",
            "Variable_5:0 (float32_ref 308) [308, bytes: 1232]\n",
            "Total size of variables: 714106\n",
            "Total bytes of variables: 2856424\n"
          ]
        }
      ],
      "source": [
        "saver = tf.train.Saver()\n",
        "!mkdir checkpointDir\n",
        "\n",
        "# Model Parameters\n",
        "slim = tf.contrib.slim\n",
        "sess=tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "sess.run(tf.local_variables_initializer())\n",
        "def model_summary():\n",
        "    model_vars = tf.trainable_variables()\n",
        "    slim.model_analyzer.analyze_vars(model_vars, print_info=True)\n",
        "    \n",
        "model_summary()\n",
        "\n",
        "# To store training and test results for visualization\n",
        "\n",
        "tr = []\n",
        "ts = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "CYc8y8uIhCHb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialized! \n",
            "Start trainning\n",
            "Epoch 1 start !\n",
            "TRain ::  9516.712  : micro  0.6655694171668833  : macro 0.6385589476535666  :  1.627406358718872\n",
            "Epoch 2 start !\n",
            "TRain ::  4180.668  : micro  0.7289769489141374  : macro 0.7018724337400638  :  0.7163894176483154\n",
            "Epoch 3 start !\n",
            "TRain ::  3632.919  : micro  0.7291686805219912  : macro 0.7020774659258335  :  0.7847762107849121\n",
            "Epoch 4 start !\n",
            "TRain ::  3538.4473  : micro  0.7294840823746084  : macro 0.7025151961923354  :  0.7254862785339355\n",
            "Epoch 5 start !\n",
            "TRain ::  3472.544  : micro  0.7285629241576378  : macro 0.7021424317482499  :  0.8774917125701904\n",
            "Epoch 6 start !\n",
            "TRain ::  3469.5164  : micro  0.7284095627929049  : macro 0.701934330236522  :  0.904247522354126\n",
            "Epoch 7 start !\n",
            "TRain ::  3448.2585  : micro  0.7288241907805195  : macro 0.7026889288675067  :  0.8291456699371338\n",
            "Epoch 8 start !\n",
            "TRain ::  3434.243  : micro  0.7274400169863402  : macro 0.7015444324540632  :  0.7900354862213135\n",
            "Epoch 9 start !\n",
            "TRain ::  3410.6594  : micro  0.7269067796610168  : macro 0.7011184666017456  :  0.7914369106292725\n",
            "Epoch 10 start !\n",
            "TRain ::  3388.464  : micro  0.7267534787514103  : macro 0.7014545012431566  :  0.783888578414917\n",
            "Epoch 11 start !\n",
            "TRain ::  3379.934  : micro  0.726492721007103  : macro 0.7011461610603373  :  0.7929210662841797\n",
            "Epoch 12 start !\n",
            "TRain ::  3369.598  : micro  0.7265470463616465  : macro 0.7011441811373721  :  0.8114962577819824\n",
            "Epoch 13 start !\n",
            "TRain ::  3368.3054  : micro  0.7254153386337899  : macro 0.7001038234069848  :  0.7944700717926025\n",
            "Epoch 14 start !\n",
            "TRain ::  3386.7314  : micro  0.7255639097744361  : macro 0.7004453204876847  :  0.8034071922302246\n",
            "Epoch 15 start !\n",
            "TRain ::  3378.4465  : micro  0.7245816496454067  : macro 0.6994677001237233  :  0.8041977882385254\n",
            "Epoch 16 start !\n",
            "TRain ::  3350.074  : micro  0.7249453564618892  : macro 0.7002491191033252  :  0.7630078792572021\n",
            "Epoch 17 start !\n",
            "TRain ::  3356.307  : micro  0.7247091097188157  : macro 0.6998723299726961  :  0.7889339923858643\n",
            "Epoch 18 start !\n",
            "TRain ::  3344.1692  : micro  0.7244919289281151  : macro 0.6995992933066386  :  0.775388240814209\n",
            "Epoch 19 start !\n",
            "TRain ::  3327.165  : micro  0.7237786148942336  : macro 0.6992549611462417  :  0.7755677700042725\n",
            "Epoch 20 start !\n",
            "TRain ::  3329.5347  : micro  0.7231763875877544  : macro 0.6989237766546754  :  0.7671003341674805\n",
            "Epoch 21 start !\n",
            "TRain ::  3324.1858  : micro  0.723375211124222  : macro 0.6989538516135387  :  0.7638671398162842\n",
            "Epoch 22 start !\n",
            "TRain ::  3284.6938  : micro  0.7219284675240711  : macro 0.6976653805195432  :  0.7670936584472656\n",
            "Epoch 23 start !\n",
            "TRain ::  3250.505  : micro  0.7221632596557611  : macro 0.6980400238622981  :  0.768303394317627\n",
            "Epoch 24 start !\n",
            "TRain ::  3225.6572  : micro  0.7218969015123571  : macro 0.6980222046018233  :  0.782886266708374\n",
            "Epoch 25 start !\n",
            "TRain ::  3228.0947  : micro  0.7219590895995391  : macro 0.6979973316740069  :  0.7757461071014404\n",
            "Epoch 26 start !\n",
            "TRain ::  3219.8376  : micro  0.7212752743081913  : macro 0.6976358874863545  :  0.8014466762542725\n",
            "Epoch 27 start !\n",
            "TRain ::  3206.5127  : micro  0.7212651273976852  : macro 0.6977231872500328  :  0.7749216556549072\n",
            "Epoch 28 start !\n",
            "TRain ::  3190.4922  : micro  0.7207467879163654  : macro 0.697006636700991  :  0.7627592086791992\n",
            "Epoch 29 start !\n",
            "TRain ::  3171.7275  : micro  0.7204252925034158  : macro 0.6968997577954062  :  0.7542994022369385\n",
            "Epoch 30 start !\n",
            "TRain ::  3185.9236  : micro  0.7204045825162556  : macro 0.6967373038498097  :  0.7547852993011475\n",
            "Epoch 31 start !\n",
            "TRain ::  3223.97  : micro  0.7205474427454667  : macro 0.6970396135051298  :  0.7579972743988037\n",
            "Epoch 32 start !\n",
            "TRain ::  3182.3777  : micro  0.7203366509195983  : macro 0.6969609818656306  :  0.7698450088500977\n",
            "Epoch 33 start !\n",
            "TRain ::  3250.9453  : micro  0.7230016614362194  : macro 0.6990779005972088  :  0.7650716304779053\n",
            "Epoch 34 start !\n",
            "TRain ::  3260.6448  : micro  0.7236807120975666  : macro 0.6992637459218722  :  0.7623162269592285\n",
            "Epoch 35 start !\n",
            "TRain ::  3172.9465  : micro  0.7212183908045977  : macro 0.6975835591245876  :  0.7561435699462891\n",
            "Epoch 36 start !\n",
            "TRain ::  3166.8247  : micro  0.7196870030773456  : macro 0.6964991727056205  :  0.7569005489349365\n",
            "Epoch 37 start !\n",
            "TRain ::  3123.6985  : micro  0.7202290294875466  : macro 0.6969712865277453  :  0.7580432891845703\n",
            "Epoch 38 start !\n",
            "TRain ::  3094.4963  : micro  0.7196407965542062  : macro 0.6966560435549713  :  0.7496135234832764\n",
            "Epoch 39 start !\n",
            "TRain ::  3078.3171  : micro  0.7193674082707713  : macro 0.6963113808059825  :  0.7530105113983154\n",
            "Epoch 40 start !\n",
            "TRain ::  3077.5398  : micro  0.7210385082910943  : macro 0.6976371184153876  :  0.7514846324920654\n",
            "Epoch 41 start !\n",
            "TRain ::  3080.194  : micro  0.7203180131054392  : macro 0.6967955500581022  :  0.757735013961792\n",
            "Epoch 42 start !\n",
            "TRain ::  3037.9902  : micro  0.7190639269406393  : macro 0.6960948396371447  :  0.7556018829345703\n",
            "Epoch 43 start !\n",
            "TRain ::  3018.6482  : micro  0.7194423813060619  : macro 0.6964604895007519  :  0.7551484107971191\n",
            "Epoch 44 start !\n",
            "TRain ::  3046.0762  : micro  0.7201465201465201  : macro 0.6968751384263738  :  0.7527627944946289\n",
            "Epoch 45 start !\n",
            "TRain ::  3035.8992  : micro  0.7193656017185429  : macro 0.696280628351295  :  0.7929291725158691\n",
            "Epoch 46 start !\n",
            "TRain ::  3055.1555  : micro  0.719111141593224  : macro 0.6960028209980444  :  0.8291411399841309\n",
            "Epoch 47 start !\n",
            "TRain ::  3071.1243  : micro  0.719995424388012  : macro 0.696882459124053  :  0.8779077529907227\n",
            "Epoch 48 start !\n",
            "TRain ::  3054.4143  : micro  0.7190998400731096  : macro 0.6959580929667841  :  0.8905696868896484\n",
            "Epoch 49 start !\n",
            "TRain ::  3003.8848  : micro  0.7190557185420258  : macro 0.696047891860788  :  0.8216354846954346\n",
            "Epoch 50 start !\n",
            "TRain ::  2985.3704  : micro  0.7184610115309967  : macro 0.6952783261933796  :  0.8052513599395752\n"
          ]
        }
      ],
      "source": [
        "#sess.run(tf.global_variables_initializer())\n",
        "print(\"Initialized! \")\n",
        "target_names = ['a','b','c','d']\n",
        "print(\"Start trainning\")\n",
        "start = time.time()\n",
        "\n",
        "testA = 0\n",
        "predsAr = []\n",
        "\n",
        "# Training for 50 epochs\n",
        "for e in range(epochs):   \n",
        "# for e in range(5):\n",
        "\n",
        "    epoch_start = time.time()\n",
        "    print(\"Epoch %d start !\" % (e + 1))\n",
        "    # for x_batch, y_batch in zip(x_train, y_train, BATCH_SIZE):\n",
        "    err = []\n",
        "    preds = []\n",
        "    trues = []\n",
        "\n",
        "    # batch sizes for crimes and anomaly and prediction\n",
        "    x_batch1 =[]\n",
        "    x_batch2 = []\n",
        "    y_batch1 = []\n",
        "    \n",
        "    # Recording error, prediction, truth values(for F1-scores)\n",
        "    for i in range(len(x_train)-80):                                      # 292 - 80 = 212 \n",
        "        i+=80\n",
        "        x_batch = x_train[i:min(len(x_train)-1,timeSize+(i))]             # i=0 -> 0-min(292-1, 10+1)=11  -> 0-11 = 12  # i=1 -> 1-min(292-1, 10+1)=11  -> 1-11 = 10\n",
        "        x_anomaly = x_train2[i:min(len(x_train)-1,timeSize+(i))]          # i=0 -> 0-min(292-1, 10+1)=11  -> 0-11 = 12  # i=1 -> 1-min(292-1, 10+1)=11  -> 1-11 = 10\n",
        "        if len(x_batch) < timeSize:\n",
        "          continue\n",
        "        x_batch = x_batch\n",
        "        x_anomaly = x_anomaly\n",
        "        y_batch = x_train[min(len(x_train)-1,timeSize+(i))].T\n",
        "        x_batch1.append(x_batch)\n",
        "        x_batch2.append(x_anomaly)\n",
        "        y_batch1.append(y_batch)\n",
        "        if (i+1)% BATCH_SIZE >0:\n",
        "          continue\n",
        "        # print(np.array(x_batch1).shape)\n",
        "        fd = {batch_x: x_batch1, anomaly_x: x_batch2, batch_y: y_batch1, keep_prob: KEEP_PROB}\n",
        "        # print(y_batch)\n",
        "        \n",
        "        l, _, oht = sess.run([loss, optimizer, one_hot_prediction], feed_dict=fd)\n",
        "        \n",
        "        for j in range(BATCH_SIZE):\n",
        "          # print(oht.shape)\n",
        "          preds.extend(np.array(oht[j]).reshape(-1,4))\n",
        "          trues.extend(np.array(y_batch1[j]).reshape(-1,4))\n",
        "        x_batch1 =[]\n",
        "        y_batch1 = []\n",
        "        x_batch2 = []\n",
        "\n",
        "        # sess.run(optimizer,feed_dict=fd)\n",
        "        err.append(l)\n",
        "    # print(sess.run(loss))\n",
        "    epoch_finish = time.time()\n",
        "    # print(preds)\n",
        "    preds = np.array(preds)\n",
        "    trues = np.array(trues)\n",
        "\n",
        "    # f1 = f1_score(y_true=y_batch, y_pred=oht, average='weighted')\n",
        "    f1 = f1_score(y_true=np.where(trues>0,1,0), y_pred=np.where(preds>0,1,0), average='micro')\n",
        "    f2 = f1_score(y_true=np.where(trues>0,1,0), y_pred=np.where(preds>0,1,0), average='macro')\n",
        "    tr.append([f1,f2])\n",
        "    # print(f1)\n",
        "    print(\"TRain :: \",np.mean(err),\" : micro \",f1,\" : macro\",f2,\" : \",epoch_finish-epoch_start)\n",
        "    # print(classification_report(y_true=trues,y_pred=preds,target_names=target_names))\n",
        "\n",
        "    ########### Predictions on test data and storing info for visualization ###########\n",
        "    if True:\n",
        "      preds = []\n",
        "      trues = []\n",
        "      x_batch1 =[]\n",
        "      y_batch1 = []\n",
        "      x_batch2 = []\n",
        "      err = []\n",
        "      for i in range(len(x_test)):\n",
        "          # i+=100\n",
        "          x_batch = x_test[i:min(len(x_test)-1,timeSize+(i))]\n",
        "          x_anomaly = x_test2[i:min(len(x_test)-1,timeSize+(i))]\n",
        "          if len(x_batch) < timeSize:\n",
        "            continue\n",
        "          x_batch = x_batch\n",
        "          x_anomaly = x_anomaly\n",
        "          y_batch = x_test[min(len(x_test)-1,timeSize+(i))].T\n",
        "          x_batch1.append(x_batch)\n",
        "          x_batch2.append(x_anomaly)\n",
        "          y_batch1.append(y_batch)\n",
        "          if (i+1)% BATCH_SIZE >0:\n",
        "            continue\n",
        "          fd = {batch_x: x_batch1,anomaly_x:x_batch2, batch_y: y_batch1, keep_prob: KEEP_PROB}\n",
        "          \n",
        "          l, acc,oht,weightSupport = sess.run([loss, accuracy,one_hot_prediction,weight_soft], feed_dict=fd)\n",
        "          \n",
        "          err.append(l)\n",
        "          # sess.run(optimizer,feed_dict=fd)\n",
        "          for j in range(BATCH_SIZE):\n",
        "            # print(oht.shape)\n",
        "            preds.extend(np.array(oht[j]).reshape(-1,4))\n",
        "            trues.extend(np.array(y_batch1[j]).reshape(-1,4))\n",
        "\n",
        "          x_batch1 =[]\n",
        "          y_batch1 = []\n",
        "          x_batch2 = []\n",
        "\n",
        "      preds = np.array(preds)\n",
        "      trues = np.array(trues)\n",
        "\n",
        "      ##################################### BY ALIF #####################################\n",
        "      # # save preds and trues in a pickle file\n",
        "      # with open('../Chicago Dataset/preds_before_threshold_1d_only_crime_data.pkl', 'wb') as f:\n",
        "      #   pickle.dump(preds, f)\n",
        "      # with open('../Chicago Dataset/trues_before_threshold_1d_only_crime_data.pkl', 'wb') as f:\n",
        "      #   pickle.dump(trues, f)\n",
        "      \n",
        "      # save preds and trues in a pickle file\n",
        "      with open('../../Comparison/TEMPORAL_PRECISION/DEEPCRIME/Outputs/1d_wo_external/preds_before_threshold.pkl', 'wb') as f:\n",
        "        pickle.dump(preds, f)\n",
        "      with open('../../Comparison/TEMPORAL_PRECISION/DEEPCRIME/Outputs/1d_wo_external/trues_before_threshold.pkl', 'wb') as f:\n",
        "        pickle.dump(trues, f)\n",
        "      ###################################################################################\n",
        "\n",
        "      y_pred=np.where(preds>0, 1, 0)\n",
        "      y_true=np.where(trues>0, 1, 0)\n",
        "\n",
        "      ##################################### BY ALIF #####################################\n",
        "      # # save preds and trues in a pickle file\n",
        "      # with open('../Chicago Dataset/preds_after_threshold_1d_only_crime_data.pkl', 'wb') as f:\n",
        "      #   pickle.dump(y_pred, f)\n",
        "      # with open('../Chicago Dataset/trues_after_threshold_1d_only_crime_data.pkl', 'wb') as f:\n",
        "      #   pickle.dump(y_true, f)\n",
        "\n",
        "      \n",
        "      # save preds and trues in a pickle file\n",
        "      with open('../../Comparison/TEMPORAL_PRECISION/DEEPCRIME/Outputs/1d_wo_external/preds_after_threshold.pkl', 'wb') as f:\n",
        "        pickle.dump(y_pred, f)\n",
        "      with open('../../Comparison/TEMPORAL_PRECISION/DEEPCRIME/Outputs/1d_wo_external/trues_after_threshold.pkl', 'wb') as f:\n",
        "        pickle.dump(y_true, f)\n",
        "      ###################################################################################\n",
        "\n",
        "      # with open('preds_NYC.pkl', 'wb') as f:\n",
        "      #   pickle.dump(preds, f)\n",
        "      # with open('trues_NYC.pkl', 'wb') as f:\n",
        "      #   pickle.dump(trues, f)\n",
        "\n",
        "      # f1 = f1_score(y_true=y_batch, y_pred=oht, average='weighted')\n",
        "      f1 = f1_score(y_true=np.where(trues>0,1,0), y_pred=np.where(preds>0,1,0), average='micro')\n",
        "      f2 = f1_score(y_true=np.where(trues>0,1,0), y_pred=np.where(preds>0,1,0), average='macro')\n",
        "\n",
        "      if testA < f1:\n",
        "        testA=f1\n",
        "        save_path = saver.save(sess, \"./modelM/model\"+str(f1)[:5]+\".ckpt\")\n",
        "        # print(classification_report(y_true=trues,y_pred=preds,target_names=target_names))\n",
        "        predsAr.append(preds)\n",
        "      ts.append([f1,f2])\n",
        "      # print(f1)\n",
        "      # print(np.mean(err),\" : micro \",f1,\" : macro\",f2,\" : \")\n",
        "      # print(weightSupport)\n",
        "      # print(classification_report(y_true=trues,y_pred=preds,target_names=target_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "KXtCiDV5cvxK"
      },
      "outputs": [],
      "source": [
        "%matplotlib notebook\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "mwcf4aSMwZIb"
      },
      "outputs": [],
      "source": [
        "# This part of code is to visualize the decay in model performance as we try to predict crimes for an extended period of time, using bootstrapping\n",
        "\n",
        "preds = []\n",
        "trues = []\n",
        "x_batch1 =[]\n",
        "y_batch1 = []\n",
        "x_batch2 = []\n",
        "err = []\n",
        "pp = []\n",
        "x_test3 = np.array(x_test)\n",
        "sub=0\n",
        "for i in range(len(x_test)):\n",
        "    # i+=100\n",
        "    i-=sub\n",
        "    x_batch = x_test3[i:min(len(x_test)-1,timeSize+(i))]\n",
        "    x_anomaly = x_test2[i:min(len(x_test)-1,timeSize+(i))]\n",
        "    if len(x_batch) < timeSize:\n",
        "      continue\n",
        "    x_batch = x_batch\n",
        "    x_anomaly = x_anomaly\n",
        "    y_batch = x_test[min(len(x_test)-1,timeSize+(i))].T\n",
        "    x_batch1.append(x_batch)\n",
        "    x_batch2.append(x_anomaly)\n",
        "    y_batch1.append(y_batch)\n",
        "    if (i+1)% BATCH_SIZE >0:\n",
        "      continue\n",
        "      sub=3\n",
        "    fd = {batch_x: x_batch1,anomaly_x:x_batch2, batch_y: y_batch1, keep_prob: KEEP_PROB}\n",
        "    l, acc,oht,weightSupport = sess.run([loss, accuracy,one_hot_prediction,weight_soft], feed_dict=fd)\n",
        "    err.append(l)\n",
        "    # sess.run(optimizer,feed_dict=fd)\n",
        "    for j in range(1):\n",
        "      # print(oht.shape)\n",
        "      preds.extend(np.array(oht[j]).reshape(-1,4))\n",
        "      trues.extend(np.array(y_batch1[j]).reshape(-1,4))\n",
        "    f1 = f1_score(y_true=np.where(np.array(trues)>0,1,0), y_pred=np.where(np.array(preds)>0,1,0), average='micro')\n",
        "    f2 = f1_score(y_true=np.where(np.array(trues)>0,1,0), y_pred=np.where(np.array(preds)>0,1,0), average='macro')\n",
        "    print(f1,\" : \",f2)\n",
        "    pp.append([f1,f2])\n",
        "\n",
        "    x_batch1 =[]\n",
        "    y_batch1 = []\n",
        "    x_batch2 = []\n",
        "    # x_test3[min(len(x_test)-1,timeSize+(i))]=np.array(oht[3]).T\n",
        "    # x_test3[min(len(x_test)-1,timeSize+(i-1))]=np.array(oht[2]).T\n",
        "    # x_test3[min(len(x_test)-1,timeSize+(i-2))]=np.array(oht[1]).T\n",
        "    x_test3[min(len(x_test)-1,timeSize+(i-3))]=np.array(oht[0]).T\n",
        "# print(preds)\n",
        "preds = np.array(preds)\n",
        "trues = np.array(trues)\n",
        "# f1 = f1_score(y_true=y_batch, y_pred=oht, average='weighted')\n",
        "f1 = f1_score(y_true=np.where(trues>0,1,0), y_pred=np.where(preds>0,1,0), average='micro')\n",
        "f2 = f1_score(y_true=np.where(trues>0,1,0), y_pred=np.where(preds>0,1,0), average='macro')\n",
        "# if testA < f1:\n",
        "  # testA=f1\n",
        "  # save_path = saver.save(sess, \"./modelM/model\"+str(f1)[:5]+\".ckpt\")\n",
        "  # print(classification_report(y_true=trues,y_pred=preds,target_names=target_names))\n",
        "  # predsAr.append(preds)\n",
        "ts.append([f1,f2])\n",
        "# print(f1)\n",
        "print(np.mean(err),\" : micro \",f1,\" : macro\",f2,\" : \")\n",
        "print(weightSupport)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4YZz9WfW54BF"
      },
      "outputs": [],
      "source": [
        "# Plotting the above data\n",
        "\n",
        "%matplotlib inline\n",
        "plt.plot(np.array(pp).T[0].T,label=\"testProg_micro\")\n",
        "plt.plot(np.array(pp).T[1].T,label=\"testProg_macro\")\n",
        "# plt.plot(np.array(ts).T[0].T,label=\"test_micro\")\n",
        "# plt.plot(np.array(ts).T[1].T,label=\"test_macro\")\n",
        "# plt.plot(ts)\n",
        "plt.title('F1 fall progressive prediction')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "hLGWbw8yss31"
      },
      "outputs": [],
      "source": [
        "# Statistical significance\n",
        "\n",
        "from scipy.stats import ttest_ind,ttest_rel,ks_2samp\n",
        "ttest_ind(predsAr[6].reshape(-1), predsAr[7].reshape(-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "r6o9L8l-LzBX"
      },
      "outputs": [],
      "source": [
        "# Seperating the model predictions to change the dimensions to (4 * days X 77) from (77 * days X 4)\n",
        "\n",
        "np.array(predsAr[8])\n",
        "a = []\n",
        "b = []\n",
        "i =0\n",
        "ar = []\n",
        "br= []\n",
        "for x,y in zip(predsAr[8],trues):\n",
        "  a.append(x)\n",
        "  b.append(y)\n",
        "  i+=1\n",
        "  if i%77==0:\n",
        "    ar.extend(list(np.array(a).T))\n",
        "    br.extend(list(np.array(b).T))\n",
        "    a = []\n",
        "    b = []\n",
        "\n",
        "br = np.array(br)\n",
        "ar = np.array(ar)\n",
        "\n",
        "print(classification_report(y_true=br,y_pred=ar))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "TL6tKdQuiC9q"
      },
      "outputs": [],
      "source": [
        "ks_2samp(predsAr[6].reshape(-1), predsAr[8].reshape(-1))\n",
        "# Statisticall significance test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "bGyITSa0l3Sb"
      },
      "outputs": [],
      "source": [
        "# Classification scores based on crime categories \n",
        "\n",
        "target_names = ['robery','burgalry','felony','grand']\n",
        "print(classification_report(y_true=trues,y_pred=predsAr[8],target_names=target_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "udPk5SfGnl1Z"
      },
      "outputs": [],
      "source": [
        "# Plotting heatmap from crime vs locality, after reshaping for better representation\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "fig2, ax2 = plt.subplots()\n",
        "fig3, ax3 = plt.subplots()\n",
        "\n",
        "min_val, max_val = 0, 15\n",
        "\n",
        "intersection_matrix =  predsAr[8][-77*3:].reshape((11*3,14*2)) #np.random.randint(0, 10, size=(max_val, max_val))\n",
        "intersection_matrix2 = trues[-77*3:].reshape((11*3,14*2))\n",
        "ax.matshow(intersection_matrix, cmap=plt.cm.Blues)\n",
        "ax2.matshow(intersection_matrix2,  cmap=plt.cm.Greens)\n",
        "\n",
        "results = [[intersection_matrix2[i][j] + intersection_matrix[i][j]  for j in range\n",
        "(len(intersection_matrix2[0]))] for i in range(len(intersection_matrix2))]\n",
        "\n",
        "ax3.matshow(results,  cmap=plt.cm.Greens)\n",
        "# ax3.matshow(intersection_matrix,  cmap=plt.cm.Greens)\n",
        "# for i in range(14*2):\n",
        "#     for j in range(33):\n",
        "#         c = intersection_matrix[j,i]\n",
        "#         ax.text(i, j, str(c), va='center', ha='center')\n",
        "\n",
        "\n",
        "# for i in range(14*2):\n",
        "#     for j in range(33):\n",
        "#         c = intersection_matrix2[j,i]\n",
        "#         ax2.text(i, j, str(c), va='center', ha='center')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "QnEcwJTirHWX"
      },
      "outputs": [],
      "source": [
        "# Heatmaps\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "fig2, ax2 = plt.subplots()\n",
        "fig3, ax3 = plt.subplots()\n",
        "\n",
        "min_val, max_val = 0, 15\n",
        "\n",
        "intersection_matrix =  predsAr[8][-77*2:].reshape((11*2,14*2)) #np.random.randint(0, 10, size=(max_val, max_val))\n",
        "intersection_matrix2 = trues[-77*2:].reshape((11*2,14*2))\n",
        "ax.matshow(intersection_matrix, cmap=plt.cm.Blues)\n",
        "ax2.matshow(intersection_matrix2,  cmap=plt.cm.Greens)\n",
        "\n",
        "results = [[intersection_matrix2[i][j] + intersection_matrix[i][j]  for j in range\n",
        "(len(intersection_matrix2[0]))] for i in range(len(intersection_matrix2))]\n",
        "\n",
        "ax3.matshow(results,  cmap=plt.cm.Greens)\n",
        "# ax3.matshow(intersection_matrix,  cmap=plt.cm.Greens)\n",
        "for i in range(14*2):\n",
        "    for j in range(33):\n",
        "        c = intersection_matrix[j,i]\n",
        "        ax.text(i, j, str(c), va='center', ha='center')\n",
        "\n",
        "\n",
        "for i in range(14*2):\n",
        "    for j in range(33):\n",
        "        c = intersection_matrix2[j,i]\n",
        "        ax2.text(i, j, str(c), va='center', ha='center')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "TrainingModels.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "ebe23dee3a652b1a649a1db8e2566ad6fae31353a6f524cb5d85b294e04ea6a1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
