{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "dN7em0OiLt2L"
      },
      "outputs": [],
      "source": [
        "# UNCOMMENT IF USING COLAB\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "YC_rm_PxMsea"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shapely.wkt\n",
        "from shapely.geometry import Point, Polygon, MultiPolygon\n",
        "import re\n",
        "\n",
        "# Reads the point-of-interest dataset from New York City\n",
        "# Important locations, classified as following, with the geographical data and date of creation/updation to records\n",
        "\n",
        "# 1 Residential\n",
        "# 2 Education Facility\n",
        "# 3 Cultural Facility\n",
        "# 4 Recreational Facility\n",
        "# 5 Social Services\n",
        "# 6 Transportation Facility\n",
        "# 7 Commercial\n",
        "# 8 Government Facility (non public safety)\n",
        "# 9 Religious Institution\n",
        "# 10 Health Services\n",
        "# 11 Public Safety\n",
        "# 12 Water\n",
        "# 13 Miscellaneous\n",
        "\n",
        "# filePOI = 'RawData/Point_Of_Interest.csv'\n",
        "# fileRegions = \"RawData/Regions.csv\"\n",
        "filePOI = \"../Dataset/Point_Of_Interest.csv\"\n",
        "fileRegions = \"../Dataset/nypp.csv\"\n",
        "\n",
        "\n",
        "poi = pd.read_csv(filePOI,low_memory=False)\n",
        "locations = pd.read_csv(fileRegions, low_memory=False)\n",
        "\n",
        "relevantColumns = ['the_geom','CREATED','FACILITY_T']\n",
        "finalColumns = ['Precincts','FACILITY_T']\n",
        "poi = poi[relevantColumns]\n",
        "\n",
        "# Reading geographical divisions of precincts to localize the POIs to their respective regions\n",
        "precincts = {}\n",
        "for index, row in locations.iterrows():\n",
        "  precincts[row['Precinct']] = shapely.wkt.loads(row['the_geom'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "H21NZuyUNUdP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "processed 1000 records!!\n",
            "processed 2000 records!!\n",
            "processed 3000 records!!\n",
            "processed 4000 records!!\n",
            "processed 5000 records!!\n",
            "processed 6000 records!!\n",
            "processed 7000 records!!\n",
            "processed 8000 records!!\n",
            "processed 9000 records!!\n",
            "processed 10000 records!!\n",
            "processed 11000 records!!\n",
            "processed 12000 records!!\n",
            "processed 13000 records!!\n",
            "processed 14000 records!!\n",
            "processed 15000 records!!\n",
            "processed 16000 records!!\n",
            "processed 17000 records!!\n",
            "processed 18000 records!!\n",
            "processed 19000 records!!\n",
            "processed 20000 records!!\n",
            "Done!!\n"
          ]
        }
      ],
      "source": [
        "# Extracting longitude and latitude details of various POIs\n",
        "# Extracting date of creation or updation to records for various POIs\n",
        "# This information is used later to localize each point\n",
        "\n",
        "lat = []\n",
        "longt = []\n",
        "date = []\n",
        "\n",
        "for idx,row in poi.iterrows():\n",
        "  location = re.split(r' |\\)|\\(' ,row['the_geom'])\n",
        "  year = int(row['CREATED'].split()[0].split('/')[-1])\n",
        "  lat = lat + [location[-2]]\n",
        "  longt = longt + [location[2]]\n",
        "  date = date + [year]\n",
        "\n",
        "poi['Year'] = date\n",
        "poi['Latitude'] = lat\n",
        "poi['Longitude'] = longt\n",
        "\n",
        "\n",
        "# Assigning the precinct number to a location in which the property is\n",
        "pos = 0\n",
        "prec = np.ndarray((poi.shape[0],))\n",
        "\n",
        "for index,row in poi.iterrows():\n",
        "\n",
        "  poo = Point(float(row['Longitude']),float(row['Latitude']))  \n",
        "  for key,val in precincts.items():\n",
        "    if poo.within(val):\n",
        "      prec[pos] = key\n",
        "      break\n",
        "\n",
        "  pos=pos+1\n",
        "  if(pos%1000 == 0):\n",
        "    print (\"processed \"+str(pos)+\" records!!\")\n",
        "    \n",
        "poi['Precincts'] = prec.astype(int)\n",
        "print(\"Done!!\")\n",
        "\n",
        "poi['Precincts'] = poi['Precincts'].astype(np.int64)\n",
        "poi = poi[poi['Precincts'] >= 0]\n",
        "\n",
        "\n",
        "# Inverted dictionaries for easy conversion of the above dataframe to a matrix that is usable later in the neural architecture\n",
        "\n",
        "# For categories of the locality, listed above\n",
        "inv_categories = {}\n",
        "n_categories = poi['FACILITY_T'].nunique()\n",
        "uniq_categories = poi['FACILITY_T'].unique()\n",
        "for i in range(n_categories):\n",
        "  inv_categories[uniq_categories[i]] = i\n",
        "\n",
        "# For geographical division of precincts\n",
        "n_precincts = poi['Precincts'].nunique()\n",
        "uniq_precincts = poi['Precincts'].unique()\n",
        "inv_prec = {}\n",
        "for i in range(n_precincts):\n",
        "  inv_prec[uniq_precincts[i]] = i\n",
        "\n",
        "poi = poi[poi['Year'] <= 2008]\n",
        "poi = poi[finalColumns]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "t05lKhiLrTkn"
      },
      "outputs": [],
      "source": [
        "matrices = np.zeros((n_precincts,n_categories),dtype=np.int64)\n",
        "exceptions = 0\n",
        "\n",
        "for idx, row in poi.iterrows():\n",
        "  try:\n",
        "    id1 = inv_prec[row['Precincts']]\n",
        "    id2 = inv_categories[row['FACILITY_T']]\n",
        "    matrices[id1][id2]= matrices[id1][id2] + 1\n",
        "  except:\n",
        "    print(\"Exception!!!\")\n",
        "    print(\"Precincts\",id1)\n",
        "    print(\"FACILITY_T\",id2)\n",
        "    exceptions = exceptions + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "colab_type": "code",
        "id": "fyLegbV1mZGq",
        "outputId": "5d4011d9-677d-4a1b-c9c8-605204cde743"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "outputPOI = 'poiMatrices'\n",
        "\n",
        "# Pickle dump this file\n",
        "file = open(outputPOI, 'wb')\n",
        "pickle.dump(matrices,file)\n",
        "\n",
        "# To load this files, use:\n",
        "# with open(filename,'rb') as toOpen:\n",
        "#     data = pickle.load(toOpen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "POI_Segregation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "ebe23dee3a652b1a649a1db8e2566ad6fae31353a6f524cb5d85b294e04ea6a1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
